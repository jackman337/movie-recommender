{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Extract and collect American movie plots from wikipedia \"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot(movie_page):\n",
    "    \"\"\"Open the given movie webpage, extract and return the plot text for this movie\"\"\"\n",
    "    plot = \"\"\n",
    "    try:\n",
    "        html = request.urlopen(movie_page)\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        h = soup.find(id=\"Plot\").find_parent()\n",
    "        elem = h.find_next_sibling()\n",
    "        while elem.name == \"p\":\n",
    "            plot = plot + elem.text\n",
    "            elem = elem.find_next_sibling()\n",
    "    except:\n",
    "        pass\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_movie_plot(movie_url_list):\n",
    "    \"\"\"Run through the movie URL list and extract movie plot for each movie\"\"\"\n",
    "    plots = []\n",
    "    for url in tqdm(movie_url_list):\n",
    "        plot = get_plot(url)\n",
    "        plots.append(plot)\n",
    "    return plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_year(year_page, base_url, movie_list):\n",
    "    \"\"\"Process the page containing movie list for one year and collect the details\"\"\"\n",
    "    # create soup object from html obtained for parsing\n",
    "    soup = BeautifulSoup(year_page, \"lxml\")\n",
    "    all_tables = soup.find_all(\"table\", class_=\"wikitable\")\n",
    "    for table in all_tables:\n",
    "        # locate the tables containing list of movies for this year\n",
    "        ths = table.find_all(\"th\")\n",
    "        if ths:\n",
    "            th_text = [th.text.strip() for th in ths]\n",
    "            # if this is our table with list of movies then extract the movie tile and URL\n",
    "            if \"Production company\" in th_text and \"Title\" in th_text:\n",
    "                trs = table.tbody.find_all(\"tr\")\n",
    "                for tr in trs:\n",
    "                    movie = tr.find_all(\"i\")\n",
    "                    # if tag found i.e. its non-empty\n",
    "                    if movie:\n",
    "                        a = movie[0].find(\"a\")\n",
    "                        if a:\n",
    "                            movie_list[\"url\"].append(base_url + a[\"href\"])\n",
    "                            movie_list[\"title\"].append(movie[0].a[\"title\"])\n",
    "    return movie_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_yearly_list(yearly_list_url, base_url):\n",
    "    \"\"\"Run through the list of years and process each year one by one and grab the movie URLS\"\"\"\n",
    "    movie_list = {\"url\": [], \"title\": []}\n",
    "    for url in yearly_list_url:\n",
    "        try:\n",
    "            html = request.urlopen(url)\n",
    "            movie_list = process_one_year(html, base_url, movie_list)\n",
    "        except:\n",
    "            pass\n",
    "    return movie_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_list_by_year(root_url, start_year, end_year):\n",
    "    \"\"\"Get the list of URL containing movie list by year\"\"\"\n",
    "    yearly_list_url = [root_url + str(year) for year in range(start_year, end_year + 1)]\n",
    "    return yearly_list_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(start_year, end_year, rel_dir_name):\n",
    "    \"\"\"main function to orchestrate data processing\"\"\"\n",
    "    # Get the list of URL containing movie list by year\n",
    "    base_url = \"https://en.wikipedia.org\"\n",
    "    root_url = base_url + \"/wiki/List_of_American_films_of_\"\n",
    "    print(f\"\\nBuilding yearly url list from year {start_year} to {end_year}...\")\n",
    "    yearly_list_url = get_url_list_by_year(root_url, start_year, end_year)\n",
    "    print(f\"DONE. Collected {len(yearly_list_url)} year urls\\n\")\n",
    "\n",
    "\n",
    "    # Run through the list of years and process each year one by one and grab the\n",
    "    # movie URLS\n",
    "    print(f\"Building movie url list from yearly url list...\")\n",
    "    movie_list = process_yearly_list(yearly_list_url, base_url)\n",
    "    print(f\"DONE. Collected {len(movie_list['url'])} movie urls\\n\")\n",
    "        \n",
    "    # Extract the plot from each movie URL\n",
    "    print(f\"Extracting movie plot from movie urls...\")\n",
    "    movie_plots = extract_movie_plot(movie_list[\"url\"])\n",
    "    \n",
    "    # Save the collected movie data as compressed parquet file and failed URLS as CSV\n",
    "    movie_list[\"plot\"] = movie_plots\n",
    "    df = pd.DataFrame(movie_list)\n",
    "    df_failed = df[df[\"plot\"] == ''].copy()\n",
    "    failed_index = df_failed.index\n",
    "    df = df.drop(failed_index, axis=0)\n",
    "    df = df.reset_index()\n",
    "\n",
    "    print(f\"Collected {len(df)} movie plots successfully.\")\n",
    "    print(f\"Failed to collect {len(df_failed)} movie plots.\")\n",
    "\n",
    "    pq_file_name = rel_dir_name + \"movie_plots.parquet\"\n",
    "    csv_file_name = rel_dir_name + \"failed_plots.csv\"\n",
    "    df.to_parquet(pq_file_name)\n",
    "    print(f\"Saved movie plots data to [{pq_file_name}]\")\n",
    "    df_failed.to_csv(csv_file_name)\n",
    "    print(f\"Saved failed movie urls to [{csv_file_name}]\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building yearly url list from year 2005 to 2005...\n",
      "DONE. Collected 1 year urls\n",
      "\n",
      "Building movie url list from yearly url list...\n",
      "DONE. Collected 193 movie urls\n",
      "\n",
      "Extracting movie plot from movie urls...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:38<00:00,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 177 movie plots successfully.\n",
      "Failed to collect 16 movie plots.\n",
      "Saved movie plots data to [../artifacts/movie_plots.parquet]\n",
      "Saved failed movie urls to [../artifacts/failed_plots.csv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_year = 2005\n",
    "end_year = 2005\n",
    "rel_dir_name = \"../artifacts/\"\n",
    "main(start_year, end_year, rel_dir_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
